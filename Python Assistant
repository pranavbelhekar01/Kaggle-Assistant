{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":726715,"sourceType":"datasetVersion","datasetId":262},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":439.345818,"end_time":"2024-04-06T05:23:33.046076","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-06T05:16:13.700258","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Python Language Assistant Using Gemma","metadata":{}},{"cell_type":"markdown","source":"![](https://ai.google.dev/static/site-assets/images/marketing/gemma.png)","metadata":{}},{"cell_type":"code","source":"import keras_nlp\nimport keras\nimport os\nimport pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sets environment variables using the `os.environ` dictionary. \n\n- `os.environ[\"KERAS_BACKEND\"] = \"jax\"`: This line sets the environment variable `KERAS_BACKEND` to `\"jax\"`. This indicates that Keras, a deep learning library, should use the JAX backend for computation.\n\n- `os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"`: This line sets the environment variable `XLA_PYTHON_CLIENT_MEM_FRACTION` to `\"1.00\"`. This environment variable is used by XLA (Accelerated Linear Algebra), a domain-specific compiler for linear algebra operations, to control the fraction of available memory that the XLA Python client will use on a TPU (Tensor Processing Unit).\n\nUsing these environment variables, the code configures the backend for Keras to use JAX and sets the memory fraction for the XLA Python client to 100%. These configurations are crucial for optimizing performance and memory usage when running deep learning models, especially on TPUs.\n","metadata":{"papermill":{"duration":0.007077,"end_time":"2024-04-06T05:16:30.160167","exception":false,"start_time":"2024-04-06T05:16:30.15309","status":"completed"},"tags":[]}},{"cell_type":"code","source":"os.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:16:30.175428Z","iopub.status.busy":"2024-04-06T05:16:30.174937Z","iopub.status.idle":"2024-04-06T05:16:30.179267Z","shell.execute_reply":"2024-04-06T05:16:30.178431Z"},"papermill":{"duration":0.013968,"end_time":"2024-04-06T05:16:30.181062","exception":false,"start_time":"2024-04-06T05:16:30.167094","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initializes a language model object named `gemma_lm` using the GemmaCausalLM class from a library, possibly keras_nlp. It creates the model from a preset configuration named \"gemma_2b_en\". This preset likely contains predefined settings, architecture configurations, and pretrained weights optimized for a specific task or language, in this case, possibly English text generation or understanding.","metadata":{"papermill":{"duration":0.010462,"end_time":"2024-04-06T05:16:30.19864","exception":false,"start_time":"2024-04-06T05:16:30.188178","status":"completed"},"tags":[]}},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:16:30.224207Z","iopub.status.busy":"2024-04-06T05:16:30.223491Z","iopub.status.idle":"2024-04-06T05:17:39.934116Z","shell.execute_reply":"2024-04-06T05:17:39.93332Z"},"papermill":{"duration":69.726261,"end_time":"2024-04-06T05:17:39.936315","exception":false,"start_time":"2024-04-06T05:16:30.210054","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:17:39.954522Z","iopub.status.busy":"2024-04-06T05:17:39.954004Z","iopub.status.idle":"2024-04-06T05:17:39.985664Z","shell.execute_reply":"2024-04-06T05:17:39.984846Z"},"papermill":{"duration":0.043537,"end_time":"2024-04-06T05:17:39.98796","exception":false,"start_time":"2024-04-06T05:17:39.944423","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function, get_prompt(query:str)->str, takes a query string as input and returns a prompt string. It formats the prompt using a template string with placeholders for instruction and response. The instruction part is filled with the input query, while the response part is left empty initially.","metadata":{"papermill":{"duration":0.008666,"end_time":"2024-04-06T05:17:40.005969","exception":false,"start_time":"2024-04-06T05:17:39.997303","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_prompt(query:str)->str:\n    template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n    prompt = template.format(\n        instruction=query,\n        response=\"\",\n    )\n    return prompt","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:17:40.024658Z","iopub.status.busy":"2024-04-06T05:17:40.024178Z","iopub.status.idle":"2024-04-06T05:17:40.028716Z","shell.execute_reply":"2024-04-06T05:17:40.027894Z"},"papermill":{"duration":0.015796,"end_time":"2024-04-06T05:17:40.030443","exception":false,"start_time":"2024-04-06T05:17:40.014647","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this code, a TopKSampler object named 'sampler' is created with a parameter k=5 indicating that it will sample from the top 5 likely tokens during text generation. The seed parameter is set to 2 for reproducibility.\n\nThen, the Gemma language model 'gemma_lm' is compiled with the sampler object using gemma_lm.compile(sampler=sampler). This likely configures the language model for text generation using the specified sampling strategy.\n","metadata":{"papermill":{"duration":0.0084,"end_time":"2024-04-06T05:17:40.047672","exception":false,"start_time":"2024-04-06T05:17:40.039272","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:17:40.066069Z","iopub.status.busy":"2024-04-06T05:17:40.065461Z","iopub.status.idle":"2024-04-06T05:17:40.07595Z","shell.execute_reply":"2024-04-06T05:17:40.075252Z"},"papermill":{"duration":0.021637,"end_time":"2024-04-06T05:17:40.077836","exception":false,"start_time":"2024-04-06T05:17:40.056199","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Before Tuning\n","metadata":{"papermill":{"duration":0.008474,"end_time":"2024-04-06T05:17:40.094921","exception":false,"start_time":"2024-04-06T05:17:40.086447","status":"completed"},"tags":[]}},{"cell_type":"code","source":"prompt = get_prompt(\"What are list comprehensions in Python?\")\nprint(gemma_lm.generate(prompt, max_length=512))","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:17:40.11351Z","iopub.status.busy":"2024-04-06T05:17:40.112884Z","iopub.status.idle":"2024-04-06T05:18:14.433016Z","shell.execute_reply":"2024-04-06T05:18:14.432031Z"},"papermill":{"duration":34.340094,"end_time":"2024-04-06T05:18:14.443541","exception":false,"start_time":"2024-04-06T05:17:40.103447","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = get_prompt(\"How to implement a stack in Python?\")\nprint(gemma_lm.generate(prompt, max_length=512))","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:18:14.463206Z","iopub.status.busy":"2024-04-06T05:18:14.462869Z","iopub.status.idle":"2024-04-06T05:18:16.245822Z","shell.execute_reply":"2024-04-06T05:18:16.244576Z"},"papermill":{"duration":1.79563,"end_time":"2024-04-06T05:18:16.248287","exception":false,"start_time":"2024-04-06T05:18:14.452657","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading Sample Dataset","metadata":{"papermill":{"duration":0.009362,"end_time":"2024-04-06T05:18:16.267858","exception":false,"start_time":"2024-04-06T05:18:16.258496","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#questions table\ndf_questions = pd.read_csv('../input/pythonquestions/Questions.csv',\n                            encoding = \"ISO-8859-1\",\n                            usecols = ['Id','Score','Title'])\n#answers table\ndf_answers = pd.read_csv('../input/pythonquestions/Answers.csv',\n                            encoding = \"ISO-8859-1\",\n                            usecols = ['ParentId','Score','Body'],#parent id links to the questions table\n                            )","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:18:16.287857Z","iopub.status.busy":"2024-04-06T05:18:16.28752Z","iopub.status.idle":"2024-04-06T05:18:55.425414Z","shell.execute_reply":"2024-04-06T05:18:55.424588Z"},"papermill":{"duration":39.150773,"end_time":"2024-04-06T05:18:55.427827","exception":false,"start_time":"2024-04-06T05:18:16.277054","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sorting for threshold score","metadata":{"papermill":{"duration":0.009044,"end_time":"2024-04-06T05:18:55.446404","exception":false,"start_time":"2024-04-06T05:18:55.43736","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_questions = df_questions[df_questions['Score'] > 0]","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:18:55.465863Z","iopub.status.busy":"2024-04-06T05:18:55.465511Z","iopub.status.idle":"2024-04-06T05:18:55.504978Z","shell.execute_reply":"2024-04-06T05:18:55.50405Z"},"papermill":{"duration":0.051486,"end_time":"2024-04-06T05:18:55.507025","exception":false,"start_time":"2024-04-06T05:18:55.455539","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_answers = df_answers[df_answers['Score'] > 0]\\\n    .sort_values('Score',ascending=False)\\\n    .drop_duplicates(subset=['ParentId'])","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:18:55.526709Z","iopub.status.busy":"2024-04-06T05:18:55.526076Z","iopub.status.idle":"2024-04-06T05:18:55.78311Z","shell.execute_reply":"2024-04-06T05:18:55.782282Z"},"papermill":{"duration":0.269772,"end_time":"2024-04-06T05:18:55.785877","exception":false,"start_time":"2024-04-06T05:18:55.516105","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa = df_questions.merge(df_answers,left_on = 'Id', right_on = 'ParentId')\\\n    .rename(columns={'Title':'Question','Body':'Answer'})[['Question','Answer','Score_x']]","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:18:55.805947Z","iopub.status.busy":"2024-04-06T05:18:55.805657Z","iopub.status.idle":"2024-04-06T05:18:56.01203Z","shell.execute_reply":"2024-04-06T05:18:56.011216Z"},"papermill":{"duration":0.219045,"end_time":"2024-04-06T05:18:56.014339","exception":false,"start_time":"2024-04-06T05:18:55.795294","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa = qa.sort_values(\"Score_x\",ascending=False).head(1000)","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:18:56.034177Z","iopub.status.busy":"2024-04-06T05:18:56.033852Z","iopub.status.idle":"2024-04-06T05:18:56.075075Z","shell.execute_reply":"2024-04-06T05:18:56.07434Z"},"papermill":{"duration":0.053398,"end_time":"2024-04-06T05:18:56.077087","exception":false,"start_time":"2024-04-06T05:18:56.023689","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = []\nfor index, row in qa.iterrows():\n    train.append(f\"Question:\\n{row['Question']}\\n\\nAnswer:\\n{row['Answer']}\")","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:18:56.096417Z","iopub.status.busy":"2024-04-06T05:18:56.096126Z","iopub.status.idle":"2024-04-06T05:18:56.160156Z","shell.execute_reply":"2024-04-06T05:18:56.15948Z"},"papermill":{"duration":0.075761,"end_time":"2024-04-06T05:18:56.161969","exception":false,"start_time":"2024-04-06T05:18:56.086208","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gemma_lm.backbone.enable_lora(rank=4)","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:18:56.18168Z","iopub.status.busy":"2024-04-06T05:18:56.181336Z","iopub.status.idle":"2024-04-06T05:18:56.545688Z","shell.execute_reply":"2024-04-06T05:18:56.544832Z"},"papermill":{"duration":0.376926,"end_time":"2024-04-06T05:18:56.548165","exception":false,"start_time":"2024-04-06T05:18:56.171239","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:18:56.569756Z","iopub.status.busy":"2024-04-06T05:18:56.569412Z","iopub.status.idle":"2024-04-06T05:18:56.602761Z","shell.execute_reply":"2024-04-06T05:18:56.601805Z"},"papermill":{"duration":0.046524,"end_time":"2024-04-06T05:18:56.604779","exception":false,"start_time":"2024-04-06T05:18:56.558255","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine tuning using LoRA","metadata":{"papermill":{"duration":0.011435,"end_time":"2024-04-06T05:18:56.627205","exception":false,"start_time":"2024-04-06T05:18:56.61577","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Limit the input sequence length to 128 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 128\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(train, epochs=1, batch_size=1)","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:18:56.650061Z","iopub.status.busy":"2024-04-06T05:18:56.649787Z","iopub.status.idle":"2024-04-06T05:23:03.669049Z","shell.execute_reply":"2024-04-06T05:23:03.66808Z"},"papermill":{"duration":247.032538,"end_time":"2024-04-06T05:23:03.670969","exception":false,"start_time":"2024-04-06T05:18:56.638431","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing after tuning","metadata":{"papermill":{"duration":0.088119,"end_time":"2024-04-06T05:23:03.849181","exception":false,"start_time":"2024-04-06T05:23:03.761062","status":"completed"},"tags":[]}},{"cell_type":"code","source":"prompt = get_prompt(\"What are list comprehensions in Python?\")\nprint(gemma_lm.generate(prompt, max_length=512))","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:23:04.030288Z","iopub.status.busy":"2024-04-06T05:23:04.029647Z","iopub.status.idle":"2024-04-06T05:23:27.389881Z","shell.execute_reply":"2024-04-06T05:23:27.388623Z"},"papermill":{"duration":23.452874,"end_time":"2024-04-06T05:23:27.391983","exception":false,"start_time":"2024-04-06T05:23:03.939109","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = get_prompt(\"How to implement a stack in Python?\")\nprint(gemma_lm.generate(prompt, max_length=512))","metadata":{"execution":{"iopub.execute_input":"2024-04-06T05:23:27.579173Z","iopub.status.busy":"2024-04-06T05:23:27.578819Z","iopub.status.idle":"2024-04-06T05:23:29.927469Z","shell.execute_reply":"2024-04-06T05:23:29.926465Z"},"papermill":{"duration":2.443191,"end_time":"2024-04-06T05:23:29.929633","exception":false,"start_time":"2024-04-06T05:23:27.486442","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}